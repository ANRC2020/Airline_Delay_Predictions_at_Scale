{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b7467cd-b285-4981-9204-ab804f7e2d5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ***Flight Delay Prediction***\n",
    "## Phase 3 Leaders: Dina Levashova and Eshan Bhatnagar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ccad8a9-8ea5-4f52-a686-f1df0f7b77de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Team 3_1**\n",
    "|     Name          | Photo                        | Email                        |\n",
    "|-------------------|------------------------------|------------------------------|\n",
    "| Dina Levashova    |    <img src=\"https://raw.githubusercontent.com/Eshan-Bhatnagar/261_project/refs/heads/main/dina_img.jpg\" width=\"200\">   | dlevashova@berkeley.edu      |\n",
    "| Abbas Siddiqui    |    <img src=\"https://raw.githubusercontent.com/Eshan-Bhatnagar/261_project/refs/heads/main/abbas_img.jpg\" width=\"200\">   | asiddiqui6@berkeley.edu      |\n",
    "| Eshan Bhatnagar   | <img src=\"https://raw.githubusercontent.com/Eshan-Bhatnagar/261_project/refs/heads/main/eshan_img.jpg\" width=\"200\"> | eshan_bhatnagar@berkeley.edu |\n",
    "| Ross Mower        |   <img src=\"https://raw.githubusercontent.com/rmower90/w261_project_images/refs/heads/main/Ross.jpg\" width=\"200\">    | rossamower@berkeley.edu      |\n",
    "| Michael Kalish    |  <img src=\"https://raw.githubusercontent.com/Eshan-Bhatnagar/261_project/refs/heads/main/michael_img.jpg\" width=\"200\"> | michael.kalish@berkeley.edu  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77a47736-131a-4792-996d-329a1ccc2d5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Abstract\n",
    "\n",
    "**Problem**: The BizOps team has found through surveys and user behavior data that fails to inform customers of delays has led to 5% churn. It was determined that the majority of churn came from misclassified delays (false positives). In these cases, the customer had to race to the gate after being informed of having extra time. This also led to a higher incidence of charge backs as well as a costly bump in support agent hours and escalations. Below is a summary of the data available for training and prediction.\n",
    "- Airline data (source: U.S. Department of Transportation)\n",
    "- Weather station data (source: U.S. National Oceanic and Atmospheric Administration)\n",
    "\n",
    "**OKRs**\n",
    "- **Objective:** To classify flights as being delayed (>15 min) at least 2 hours before the scheduled flight departure time.\n",
    "- **Key result:** The following key results are deemed to be realistic, measurable and achievable:\n",
    "  - Fbeta (beta=0.5) > 0.60\n",
    "  - Minimize overfitting to generalize to unseen data\n",
    "\n",
    "**Feature engineering:**\n",
    "- Transformed features to extract relevant information\n",
    "- Augmented feature set with time-based, graph-based, and event-based features.\n",
    "\n",
    "**Models tested:**\n",
    "- Logistic Regression\n",
    "- Naive Bayes\n",
    "- Multi-Layer Perceptron\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "\n",
    "**Results:** \n",
    "  - **Best model:** Naive Bayes for performance and generalizability\n",
    "  - **Training:** Fbeta is fairly consistent at 0.52. While this does not meet our OKR (>0.6), it does provide a point on which we can iterrate from what appears to be a fairly generalizable result.  \n",
    "  - **Testing:** Fbeta is 0.52, which is slightly lower than our target but consistent with the training results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8883431-0d6d-440b-abea-319b0d3a13b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Description\n",
    "The original data source of this project was called the On Time Performance and Weather (OTPW) dataset which was provided to us by the U.S. Department of Transportation and the U.S. National Oceanic and Atmospheric Administration. This joint dataset combined flight and airline information as well as weather and weather station information from 2015 to 2019. In earlier phases of the project, we did a preliminary analysis on a subset of 3 months in the year of 2015 followed by a subset of 12 months for that year. During this analysis, we found that half the rows were duplicated, and we felt that we could also do a better job of capturing weather information by joining airports to their closest weather stations and joining the weather observations available two hours prior to each flight’s expected departure time block and date. Additional details on our custom joint procedure for the data can be found in the **Custom Join + Comparison to OTPW** section. \n",
    "\n",
    "Our custom joint dataset has a total of 30,186,150 distinct rows, representing flights from 2015 to 2019. Our training data consisted of 23,108,496 rows representing flights from 2015 to 2018 with the remaining 7,077,654 rows being used in our test set and representing flights in 2019. Looking at our training data, we found that 18,676,037 (approximately 82%) flights were labeled as not delayed, whereas only 4,153,318 (approximately 18%) flights were labeled as delayed. This indicates a heavy class imbalance and we addressed this issue by undersampling the majority class to balance the label distribution before checkpointing the dataset to be used for our binary classification models. Additionally, we found that only 286,319 (approximately 1.3%) flights were cancelled, so we decided to only include cancelled flights that were also labeled as delayed.\n",
    "\n",
    "The **Data Dictionary** section below captures the final features and their metrics from our custom joint dataset that were passed into our models. Additional information on these features is described in the feature engineering and feature augmentation sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd628a70-4866-4525-9f7b-1ab5a38b14ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Sources\n",
    "\n",
    "| Name | Type | Source | URL Link| Description| Approx. File Size (Mb) | Provided vs. Additional Download| \n",
    "|----------|----------|----------|----------|----------|----------|----------|\n",
    "| Reporting Carrier On-Time Performance (1987-present)    | Flights | U.S. DOT     | https://www.transtats.bts.gov/Fields.asp?gnoyr_VQ=FGJ     | Airline flights data subsetted to 2015-2019     | 2584 | Provided     |\n",
    "| Quality Controlled Local Climatological Data    | Weather | U.S. NOAA     | https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ncdc:C00679     | Climate data for temperature, precipitation, and winds at 3-hourly intervals     | 25,075 |Provided     |\n",
    "| Airport dataset    | Airport | U.S. DOT     | https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ncdc:C00679     | Dataset for airport information and metadata     | 54.6| Provided     |\n",
    "| Airport codes    | Airport | U.S. DOT     | https://datahub.io/core/airport-codes     | Dataset for airport codes    | 8.3  | Provided     |\n",
    "| Airport coordinates    | Airport | U.S. DOT     | https://www.transtats.bts.gov/Fields.asp?gnoyr_VQ=FLL     | Geographical information for airports    | 0.6 | Additional Download     |\n",
    "| Flights and weather (OTPW)    | All | U.S. DOT / NOAA     | ---    | Joined table using datasets provided above    | 6526 | Provided     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e4c8846-49aa-45be-ae92-6e760b5c9a6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Dictionary\n",
    "| Category | Feature | Description| Type | Categorical/Numerical| Mean | Std. Dev. | % Null | % Outliers | \n",
    "|----------|----------|----------|----------|----------|----------|----------|----------| ---------- | \n",
    "| Date    |  QUARTER     | Fiscal Quarter     | String     | Categorical | NA | NA | 0 | NA |\n",
    "| Date    |  MONTH     | Month of Flight     | String     | Categorical | NA | NA | 0 | NA |\n",
    "| Date    |  DAY_OF_MONTH     | Day of Month     | String     | Categorical | NA | NA | 0 | NA |\n",
    "| Date    |  DAY_OF_WEEK    | Day of the week     | String     | Categorical | NA | NA | 0 | NA |\n",
    "| Airline    |  OP_UNIQUE_CARRIER    | Unique identifier for airline carrier     | String     | Categorical | NA | NA | 0 | NA |\n",
    "| Trip    |  DEP_TIME_BLK     | Scheduled departure time hour block  | String     | Categorical | NA | NA | 0 | NA |\n",
    "| Trip    |  ARR_TIME_BLK     | Scheduled arrival time hour block | String     | Categorical | NA | NA | 0 | NA |\n",
    "| Trip    |  CRS_ELAPSED_TIME     | Scaled scheduled flight time | Integer     | Numerical | 141.03 | 70.41 | <0.001% | 1.51% | \n",
    "| Trip    |  DISTANCE     | Scaled flight distance | Integer     | Numerical | 804.65 | 566.45 | 0 | 1.23% | \n",
    "| Weather    |  wind_speed    | Describes wind speed within the hour in knots     | Double     | Numerical | 18.82 | 121.97 | 3.8% | 1.46% |\n",
    "| Weather    | wind_direction     | Describes wind direction within the hour in degrees    | Double     | Numerical | 193.24 | 98.92 | 19.5% | 0 |\n",
    "| Weather    |  gust_speed     | Describes increases in wind speed within the hour in knots    | Double     | Numerical | 4.66 | 1.31 | 3.8% | 1.467% |\n",
    "| Weather    |  ceiling height below_10000     | One hot encoded variable for if the cloud ceiling height is below 10,000 meters   | Double     | Categorical | NA | NA| 0 | NA |\n",
    "| Weather    |  ceiling height between_10000_20000     | One hot encoded variable for if the cloud ceiling height is between 10,000 meters and 20,000 meters   | Double     | Categorical | NA | NA | 0 | NA |\n",
    "| Weather    |  ceiling height above_20000     | One hot encoded variable for if the cloud ceiling height is above 20,000 meters   | Double     | Categorical | NA | NA | 0 | NA |\n",
    "| Weather    |  visibility    | Describes visibility distance within the hour in kilometers   | Double     | Numerical | 30.495 | 122.94 | 3.8% | 1.52% |\n",
    "| Weather    |  temperature    | Describes temperature within the hour in celsius   | Double     | Numerical | 30.71 | 119.93 | 3.8% | 1.44% |\n",
    "| Weather    |  dew_point    | Describes dew point temperature within the hour in celsius   | Double     | Numerical | 24.73 | 124.99 | 3.8% | 1.55% |\n",
    "| Weather    |  sea_level_pressure    | Describes sea level pressure within the hour in hectopascals   | Double     | Numerical | 1016.96 | 6.67 | 13.5% | 0.77% |\n",
    "| Historical    |  HIST_ARR_FLT_NUM    | Augmented feature describing which historical arriving flight   | Double     | Categorical | NA | NA | 0 | NA |\n",
    "| Historical    |  HIST_DEP_FLT_NUM    | Augmented feature describing which historical departing flight   | Double     | Categorical | NA | NA | 0 | NA |\n",
    "| Holiday    |  isHoliday    | Augmented feature describing whether flight date falls within the window of a holiday   | Double     | Categorical | NA | NA | 0 | NA |\n",
    "| Weather    |  specWeather    | Augmented feature describing whether there is a special weather event (e.g. thunderstorm)   | Double     | Categorical | NA | NA | 0 | NA |\n",
    "| Historical    |  HIST_ARR_DELAY    | Augmented feature describing delay time of a historical arriving flight   | Double     | Numerical | 2.18 | 38.87 | 0.74% | 1.90% |\n",
    "| Historical    |  HIST_DEP_DELAY    | Augmented feature describing delay time of a historical departing flight   | Double     | Numerical | 7.90 | 38.02 | 0.40% | 1.74% |\n",
    "| Airport    |  origin_yr_flights    | Augmented feature describing flight traffic at origin airports   | Double     | Numerical | 122309.09 | 99208.49 | 0.01% | 0 |\n",
    "| Airport    |  dest_yr_flights    | Augmented feature describing flight traffic at destination airports   | Double     | Numerical | 122291.22 | 99213.92 | 0.01% | 0 |\n",
    "| Airport    |  pagerank    | Augmented feature measuring airport centrality   | Double     | Numerical | 5.51 | 4.74 | 4.46% | 0 |\n",
    "| Label  |  DEP_DEL15     | Indicator if departure delay is 15 minutes or more (1 if true, 0 if false).     | Double     | Categorical | NA | NA | 1.2% | NA |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac823725-0e1f-425e-874c-35245bb79fe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Custom Join + Comparison to OTPW\n",
    "\n",
    "Our custom join aimed to combine the weather data that was avalaible 2 hours before each expected departure time block from the closest station to the departing airport to each flight in the airlines dataset. The goal was to provide us with more relevant weather metrics that can then be lagged alongside other flight's past information to better inform the models about the conditions encountered by airplanes in their prior trips.\n",
    "\n",
    "Both airport and weather datasets were important for our prediction analysis. While we had access to the `Pre-joined` OTPW dataset, we did not know how this join was performed and therefore can not speak to its accuracy. Therefore, we decided to create a `Custom` joined dataset using the workflow below:\n",
    "\n",
    "**Workflow:**\n",
    "\n",
    "1. Augment the airlines dataset with longitude-latitude pairs for each airport\n",
    "2. Create another column for delayed flight departure blocks by 2 time blocks (i.e 10:00-10:59 -> 8:00-8:59)\n",
    "3. Filter the weather dataset by report type (Kept FM-15 and SOA for air and ground weather observations respectively)\n",
    "4. Augment the weather dataset with the timeblock that corresponds to the exact time of its measurements (i.e. 12:06 -> 12:00-1:00)\n",
    "5. Leverage Haversine distance (spherical distance formula) to find the closest weather station to each airport (Save as Closest_Station)\n",
    "6. Left join Closest_Station against airlines dataset\n",
    "7. Left join weather dataset against airlines dataset based on date, time blocks (delayed for airlines), and closest weather station\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Eshan-Bhatnagar/261_project/refs/heads/main/cust.png\">\n",
    "\n",
    "## Comparison between OTPW and Custom join\n",
    "To validate our `Custom` join against the `Pre-joined` dataset, we compared the number of columns, rows, and missing data between the 3-month (Q1 2015) and 12-month (2015) datasets, weighing more heavily results from the 12-month dataset. These summary statistics are shown in the table below. While the 3-month dataset for the `Custom` join has fewer distinct rows or data observations by approximately 5% compared to the `Pre-joined` dataset, the `Custom` 12-month dataset has approximately 17% more distinct observations. Most of the mismatching columns are either repetitive columns, columns with different names, or columns that have been deemed less important. However, there were certain columns, like REM and REPORT_TYPE, from the weather dataset that were included in the `Custom` dataset and excluded from the `Pre-joined` dataset but still have important information about weather conditions.\n",
    "\n",
    "### Missing values comparison\n",
    "| Dataset | Join Type | Count All Rows | Count Distinct Rows | Contains Duplicate Rows | Count All Columns |\n",
    "|----------|----------|----------|----------|----------|----------|\n",
    "| 3 month    | Pre-joined | 1401363** | 1401363 | False | 216 |\n",
    "| 3 month    | Custom     | 1324970 | 1324970 | False | 239 |\n",
    "| 1 year    | Pre-joined     | 11623708 | 5811854 | True | 216 |\n",
    "| 1 year    | Custom     | 7074200** | 7074200 | True | 239 |\n",
    "\n",
    "**During the processing of our custom datasets, we dropped the duplicate rows and thus do not have numbers for the original dataset that contained duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50f12343-3027-4d70-bc45-080bb559772a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb545580-1e0e-418d-9098-ce3599cd9009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We performed EDA on multiple features to determine which ones to keep for our model pipelines. We plotted the distribution of delayed and non delayed flights over various features such as days of the week, hours of the day, months, airports, and airlines to see which features would be particularly influential for predicting flight delays. For example when looking at the distribution of flights by airline, we can see that there is not an even distribution and airlines like Southwest and Delta tend to have a lot more flights in general. When looking at the percentage of delays for each airline, we can also see that there is a lot of variability in percent delays even within the top ten airlines. This makes the unique carrier a useful feature for delay prediction.  \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Eshan-Bhatnagar/261_project/refs/heads/main/airline_pct.png\">\n",
    "\n",
    "On the other hand when plotting the distribution of flights over days of the week, we can see the distribution is mostly evenly spread out across each day with not many standout days having significantly more flights or delay percentage. Therefore, a feature like day of the week may not be as useful for delay prediciton.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Eshan-Bhatnagar/261_project/refs/heads/main/weekday_pct.png\">\n",
    "\n",
    "The weather data originally obtained was not ideal for analysis, with many values combined into one string. For example, the \"WND\" field had a single string to represent wind direction, speed, and quality of the measurement. \"TEMP\" was missing decimal points, with a value such as \"+104\" representing \"10.4 degrees Celsius.\" Therefore, we first parsed this data to extract useful numerical features, particularly wind direction, wind speed, temperature, gust speed, visibility, ceiling height, sea level pressure, and dew point. We dropped outliers, which were misreadings and represented in the data as max values (i.e. 999 for wind speed). \n",
    "\n",
    "Additional EDA was done to plot the distributions of these newly parsed features. A few had fairly normal distributions such as wind direction, temperature, and sea level pressure as can be seen below.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Eshan-Bhatnagar/261_project/refs/heads/main/slp.png\">\n",
    "\n",
    "On the other hand, most other features had heavily skewed distributions with flight distance and wind speed serving as good examples. This indicated the need to transform our features to normalize them before passing them to the models in our pipelines. We opted to use MinMaxScaler for this purpose.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Eshan-Bhatnagar/261_project/refs/heads/main/windspeed5y.png\">\n",
    "<img src=\"https://raw.githubusercontent.com/Eshan-Bhatnagar/261_project/refs/heads/main/distance5y.png\">\n",
    "\n",
    "One unusual feature distribution we found was for the cloud ceiling height feature. The majority of the values are above 20,000 meters, while there is also a small skewed distribution of heights from 0 to 10,000 meters. Instead of performing transformations on this distribution, we decided to create bins of the cloud ceiling heights and convert it into a categroical one hot encoded feature with binary values representing whether the cloud ceiling height is between 0 to 10,000 meters, between 10,000 to 20,000 meters, or above 20,000 meters.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Eshan-Bhatnagar/261_project/refs/heads/main/ceiling_height.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0dacb107-0051-40b7-b351-fe1b20defaa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature Augmentation\n",
    "From insights gained in our EDA and in an effort to extract new useful patterns from our data while limiting cardinality, we developed new features that captured historic flights, centrality, holidays, special weather events, and airport traffic volume.\n",
    "\n",
    "## Historic Flights \n",
    "Data leakage occurs in time series problems when information from the future is used improperly during training. An example of data leakage would be using data after the two hour window between our prediction and the estimated departure time. We addressed this issue with the augmentation of our historical features as well as with our cross-validation approach which is described in the **Sampling Strategy** section.\n",
    "\n",
    "We know that if we track a flights progression in time, the status of that flight's previous delays will likely affect its scheduled departure. Therefore, we added the arrival and departure times and delays of the previous flight to the scheduled flight. Once we converted all times to UTC, we found the difference between the previous flight's arrival and departure times with the current flight's scheduled departure time. We then masked out information that was within the two-hour window from when we needed to make the prediction to prevent data leakage. If the previous flight's information was masked due to the overlap with the 2-hour window, we used information from the flight prior in order to not have any null values. We created an additional feature that kept track of the number of flights lags - how many flights we had to go back in order to find a value outside the 2-hour window. An example of creating a historic arrival delay feature is presented below for a flight that is departing from JFK and arriving in BOS with historic flights from LAX, DIA, and DCA.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Eshan-Bhatnagar/261_project/refs/heads/main/map.jpg\" width=\"500\"/>\n",
    "\n",
    "\n",
    "#### Step 1 - lag arrival time and delay\n",
    "\n",
    "| DEP_CITY | ARR_CITY| SCHED_DEP_TIME_UTC | PREV_ARR_TIME_1_UTC | PREV_ARR_DELAY_1 | PREV_ARR_TIME_2_UTC | PREV_ARR_DELAY_2 | PREV_ARR_TIME_3_UTC | PREV_ARR_DELAY_3 |\n",
    "|----------|----------|----------|----------|----------|----------|----------|----------|----------|\n",
    "| JFK   | BOS| 20170107T:20:00:00 | 20170107T:19:40:00 | 15 | 20170107T:18:20:00 | 0 | 20170107T:12:00:00 | 50 |  \n",
    "\n",
    "#### Step 2 - Difference previous arrival times and scheduled departure times\n",
    "\n",
    "| DEP_CITY | ARR_CITY| SCHED_DEP_TIME_UTC | PREV_ARR_1_DIFF_MIN | PREV_ARR_DELAY_1 | PREV_ARR_2_DIFF_MIN | PREV_ARR_DELAY_2 | PREV_ARR_2_DIFF_MIN | PREV_ARR_DELAY_3 |\n",
    "|----------|----------|----------|----------|----------|----------|----------|----------|----------|\n",
    "| JFK   | BOS| 20170107T:20:00:00 | 20 | 15 | 100 | 0 | 360 | 50 |  \n",
    "\n",
    "#### Step 3 - Mask out information within 2-hour prediction window\n",
    "\n",
    "| DEP_CITY | ARR_CITY| SCHED_DEP_TIME_UTC | PREV_ARR_1_DIFF_MIN | PREV_ARR_DELAY_1 | PREV_ARR_2_DIFF_MIN | PREV_ARR_DELAY_2 | PREV_ARR_2_DIFF_MIN | PREV_ARR_DELAY_3 |\n",
    "|----------|----------|----------|----------|----------|----------|----------|----------|----------|\n",
    "| JFK   | BOS| 20170107T:20:00:00 | 20 | NaN | 100 | NaN | 360 | 50 |\n",
    "\n",
    "#### Step 4 - Create previous arrival delay feature\n",
    "\n",
    "| DEP_CITY | ARR_CITY| SCHED_DEP_TIME_UTC | PREV_ARR_DELAY | PREV_ARR_FLIGHT |\n",
    "|----------|----------|----------|----------|----------|\n",
    "| JFK   | BOS| 20170107T:20:00:00 | 50 | 3 |\n",
    "\n",
    "### Historic Flight EDA\n",
    "The figure below shows a breakdown of the association between the scheduled flight delay and whether one of three previous flights was delayed using the training data (2015-2018). Therefore, 55.8% of scheduled flight delays are associated with historic departure delays, further confirming the importance of this new feature.\n",
    " \n",
    "<img src=\"https://raw.githubusercontent.com/rmower90/w261_project_images/refs/heads/main/historic_delays.jpg\" width=\"800\" style=\"display: block; margin: 0 auto\"/>\n",
    "\n",
    "## Centrality\n",
    "We created a feature to measure the importance of airport connectivity. To do so, we implemented a `PageRank` graphing algorithm with the nodes being the airports and edges being flights connecting the airports. The PageRank score developed from these graphs was used as a feature for departure airports in our prediction. The challenge of implementing this algorithm was developing a graph that is relevant for the airport connectivity as it varies in time without incurring data leakage, since we can assume airport connectivity varies from week to week. Therefore, we implemented the pagerank algorithm using the `GraphFrame` python packages on a quarterly basis and joined each pagerank metric to the previous quarter to prevent data leakage. Therefore, our Q1 of 2015 contained nulls and Q2 of 2015 contained the pagerank metrics of the departure airport from Q1 2015. \n",
    "\n",
    "### Centrality EDA\n",
    "The figure below shows the spatial distribution of the mean and the Coefficient of Variation (CV) of the quartlerly pagerank metrics for each airport using the training data (2015-2018), highlighting the relative importance and variability of airports with the most connecting flights.\n",
    "$$CV  = \\mu/\\sigma$$\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/rmower90/w261_project_images/refs/heads/main/pagerank.jpg\" width=\"1000\" style=\"display: block; margin: 0 auto\"/>\n",
    "\n",
    "## Holiday Travel Indicator\n",
    "\n",
    "In order to better capture the busier flying times of the year around the holidays, we created an indicator feature called “isHoliday” that had a value of 1 if the flight date fell within a 7-day window around a holiday (including 3 days before and after). The list of holidays was determined based on the paid federal US holidays sourced from Azure Public Datasets.\n",
    "\n",
    "### Holiday Travel Indicator EDA\n",
    "The figure below shows a breakdown of the association between the scheduled flight delay and the new \"isHoliday\" feature using the training data (2015-2018). Therefore, 24.6% of scheduled flight delays are associated the \"isHoliday\" feature, further confirming the importance of this new feature.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/rmower90/w261_project_images/refs/heads/main/holiday_delays.jpg\" width=\"800\" style=\"display: block; margin: 0 auto\"/>\n",
    "\n",
    "\n",
    "## Special Weather Indicator\n",
    "The weather data included 13 different fields describing special weather conditions such as snow, ice, thunderstorms, etc. To simplify the fields while capturing the information relevant to flight delays, we consolidated them into a single indicator feature called “specWeather.” At first, we made this a numerical feature that counted the number of non-empty values in the 13 fields for each row. However, after seeing that the distribution for this new feature was skewed, with most value being either 0, 1, or 8, we decided it was more meaningful to represent this information as a categorical feature, indicating that a special weather condition is present (1) or not (0). \n",
    "\n",
    "### Special Weather Indicator EDA\n",
    "The figure below shows a breakdown of the association between the scheduled flight delay and new \"“specWeather\" feature using the training data (2015-2018). Therefore, 24.6% of scheduled flight delays are associated the \"“specWeather\" feature, further confirming the importance of this new feature.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/rmower90/w261_project_images/refs/heads/main/specialweather_delays.jpg\" width=\"800\" style=\"display: block; margin: 0 auto\"/>\n",
    "\n",
    "## Airport Traffic Volume\n",
    "Our EDA showed that the origin and destination airports may be valuable features for our model training. However, with one-hot-encoding, this feature would have resulted in high cardinality and reduced the efficiency of our training. Instead, we replaced these features with the average number of flights passing through the origin and destination airports per year. Therefore, we were able to represent the relative size of each airport while reducing the total number of features passed into each model. As we used the four years of training data to calculate these values, some data leakage was present during the cross-validation portion of the model development but it was no longer the case for the final blind testing.\n",
    "\n",
    "### Airport Traffic Volume EDA\n",
    "The figure below shows the average annual count of flights for the top 10 origin airports using the training data (2015-2018).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/rmower90/w261_project_images/refs/heads/main/top_airports.jpg\" width=\"600\" style=\"display: block; margin: 0 auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d749650c-46be-485e-a01b-6b8fe88b879b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fad302f-6b58-4aaa-a8bd-55f5cf1d8925",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Metrics for Evaluating Success\n",
    "\n",
    "### Primary metric: F-beta score\n",
    "$$F_\\beta = \\frac{(1 + \\beta^2) \\text{tp}}\n",
    "                 {(1 + \\beta^2) \\text{tp} + \\text{fp} + \\beta^2 \\text{fn}}$$\n",
    "Implementation: sklearn.metrics.fbeta_score\n",
    "\n",
    "The F-beta score measures the harmonic mean of precision and recall, with the optimal score being 1 and worst score being 0. It allows for weighting of precision and recall based on use case. The \\\\(\\beta\\\\) parameter represents the ratio of importance of recall to precision, with a value above 1 placing more weight on recall and a value less than 1 placing more weight on precision. \n",
    "\n",
    "Since we are assuming a false positive is more detrimental than a false negative, we placed a higher weight on precision than recall, setting our \\\\(\\beta\\\\) value to 0.5.\n",
    "\n",
    "### Other metrics: precision and recall\n",
    "$$Precision = \\frac{TP}{TP+FP}$$\n",
    "Implementation: sklearn.metrics.precision_score\n",
    "\n",
    "$$Recall = \\frac{TP}{TP+FN}$$\n",
    "Implementation: sklearn.metrics.recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2684407-472c-48dc-b61e-de98d2d89db2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Leakage\n",
    "Data leakage is a situation where a model inadvertently is provided information during training that would not be available during the time of prediction. This is especially important in timeseries models and often results in overfitting and models that are poor at generalizing. While it is very difficult to completely rid the models of all data leakage, significant effort was taken to minimize it through this work. We already discussed ways that we handled data leakage during the custom join and feature augmentation sections. In subsquent sections, we will continue to highlight areas where precautions were taken to minimize data leakage and potential areas of data leakage concern. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e00ccd4-56d6-4a6f-b8dc-6356c3a6b738",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sampling Strategy\n",
    "In order to develop our models, we used a sliding window cross-validation approach, where we broke the first 4 years of the data (2015-2018) into 8 even folds based on the number of rows in the data. This equated to about 6 months of data per fold to capture some seasonality trends but also be of a manageable size to allow for efficient iteration in our model development. The first 80% of each fold was used for training and the final 20% was reserved for validation testing to reduce data leakage. Preliminary testing with a smaller subset of data (only the first three quarters of 2015) suggested an overlap of 10% between the folds should be appropriate to avoid harsh transitions between folds that may otherwise fail to capture key data trends. The training and validation metrics from each fold were then weighted according to the fold’s recency (multiplying by its fold number and taking the average of all the folds) as more recent folds should better reflect the state of the blind test set (2019). At the end of validation, the entire training dataset (2015-2018) was used to train the best version of each model and the final year (2019) was used for final evaluation.\n",
    "<img src=\"https://raw.githubusercontent.com/rmower90/w261_project_images/refs/heads/main/sliding_window.jpg\"/>\n",
    "\n",
    "Below is a diagram of how our data was split into each fold.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Eshan-Bhatnagar/261_project/refs/heads/main/crossval.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c636ee6-6795-4e11-a204-0c4eae2fb2f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Machine Learning Models Tested\n",
    "\n",
    "| Algorithm | Type | Purpose | Implementation | Loss Function | Loss Function Formula | \n",
    "|----------|----------|----------|----------|----------|----------|\n",
    "| Logistic Regression | Classification | Baseline and Model 1 | pyspark.ml.classification.LogisticRegression | log loss | $$-{(y\\log(p) + (1 - y)\\log(1 - p))}$$ |\n",
    "| Naive Bayes  | Classification | Model 2 | pyspark.ml.classification.NaiveBayes | negative joint log-likelihood | $$-\\log(p(X, Y))$$ |\n",
    "| Multi-layer perceptron | Classification | Model 3 | pyspark.ml.classification.MultilayerPerceptronClassifier | log loss | $$-{(y\\log(p) + (1 - y)\\log(1 - p))}$$ |\n",
    "| Random Forest | Classification | Model 4 | pyspark.ml.classification.RandomForestClassifier | log loss | $$-{(y\\log(p) + (1 - y)\\log(1 - p))}$$ |\n",
    "| XGBoost  | Classification | Model 5 | SparkXGBClassifier | log loss | $$-{(y\\log(p) + (1 - y)\\log(1 - p))}$$ |\n",
    "\n",
    "\n",
    "\n",
    "Our focus was to classify whether or not a flight is delayed by 15 or more minutes to provide passengers with a courtesy warning 2 hours prior to departure. Therefore, we opted to train classification models to most accurately and efficiently make this prediction. We implemented logistic regression to both serve as our baseline model and provide insight into which features may be most important in contributing to delays. To improve on the baseline, we added our augmented features and implemented fine-tuned logistic regression, Naive Bayes, a Random Forest classifier, a Multilayer Perceptron classifier and an XGBoost classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2772eca8-f968-465d-9d3b-6809fac8bb6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Hyperparameter Tuning with Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5afe90f-1b7a-46ce-bcdb-7b1ca19aad40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "We decided to leverage Optuna for the following reasons:\n",
    "\n",
    "- Random search with suggestion of hyperparameters \n",
    "- Pruning feature for determining when trials are going the wrong way\n",
    "- **Customization of training and validation loops; We were able to constitute what falls into each split explicitly; Crossvalidator would shuffle our values around**\n",
    "- Able to look at inter-fold metrics more easily compared to other frameworks\n",
    "- Study object automatically tracked the trials, metrics, and performance over time\n",
    "- The study object comes with many useful plots for analysis (i.e contour plot)\n",
    "- We could see what took the longest and determine the right resource management\n",
    "- We could run jobs in parallel by specifying 'n_jobs' parameter\n",
    "- We could easily to implement a weighted fold calculation for each trial within our Optuna framework\n",
    "\n",
    "Conclusion: Oputna was a very conveinent and all around powerful package for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07d4543c-8664-426e-8498-0cc798e7c636",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Baseline Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e208e98-56d0-4078-ab6b-54571444127b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We chose to use logistic regression for our baseline model and initial evaluation of features and pre-processing techniques due to its simplicity, short runtime, and interpretability. Initial testing with a 1-year subset of our model helped us develop our initial modeling pipeline and determine which configurations would be most interesting to test on our main data. We decided that we needed strategies to scale numerical features, impute null values, handle outliers, and manage the first quarter of 2015 (for which the PageRank feature is empty). To keep our baseline simple and straightforward, we used only our initial set of features (excluding augmented features) and scaled all numerical values with the MinMax scalar, based on the skewed distributions of most of the features. We also set the max iterations to 10 as we found that a higher value did not significantly affect the results but adversely affected the runtime. Experimentation on the 1-year dataset yielded the following train/validation results.\n",
    "\n",
    "| Training/Validation  | Fbeta | Precision | Recall | \n",
    "|------|-------|-----------|--------|\n",
    "| Imputing with mean | 0.63/0.56  | 0.62/0.58      | 0.68/0.59   |\n",
    "| Dropping outliers   | 0.64/0.57  | 0.63/0.59      | 0.70/0.59   |\n",
    "| Max Iterations = 50   | 0.63/0.56  | 0.62/0.58      | 0.68/0.59   |\n",
    "\n",
    "Our options to impute null values were either to drop all rows with null values or to impute them with the mean/median of the rest of the data. Imputing with the mean/median yielded similar results so we decided to use the mean for imputation in our pipeline. Our options to handle outliers (everything outside the interquartile range of 0.25-0.75) were either to keep them or drop them, and though the results were comparable, we decided to keep outliers due to the non-normal distribution of most of our features. Our options to handle the PageRank feature (null for the first quarter of 2015) was to either impute the values with the mean of the rest of the training data (which would have contributed to data leakage) or to drop the first quarter altogether. We opted to go with the latter option as it did not yield worse results, reduced data leakage, and was not as relevant from a time-series perspective to predict the state of 2019 as the more recent years.\n",
    "After helping determine this standardized pipeline for our model development, we tested our baseline model on the blind test set to yield the following results:\n",
    "\n",
    "| Dataset  | Fbeta | Precision | Recall |\n",
    "|------|-------|-----------|--------|\n",
    "| Train    | 0.62  | 0.60      | 0.67   |\n",
    "| Test    | 0.30  | 0.26      | 0.66   |\n",
    "\n",
    "These results were not surprising, as the test set is imbalanced (reflecting the true state of delayed flights) and it was easier for the model to predict on-time flights (recall) than delayed flights (precision), leading to a lower fbeta score which was weighted heavier on precision. This baseline model was also not subjected to any hyperparameter tuning nor did it include any of our augmented features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db903eae-d420-45eb-9422-49e3e68bfd10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Machine Learning Pipeline\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/rmower90/w261_project_images/refs/heads/main/w261_ML_Pipeline.jpg\" alt=\"Machine Learning Pipeline\" width=\"1200\" height=\"1000\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81445eb2-2755-46bb-adf8-c4c28d063434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Stage 1: Prepare Train/Val**\n",
    "\n",
    "- Reach concensus on pre-processing and scope of the raw data through EDA and random sampling while addresssing any potential leakage concerns\n",
    "\n",
    "**Stage 2: Prepare Features**\n",
    "\n",
    "- Based on features selected and their respective preprocessing, further transform features (i.e discretizing, rounding, binning) and drop or impute nulls based on feature engineering strategy.\n",
    "\n",
    "**Stage 3: Prepare Piplines**\n",
    "\n",
    "- Explore the hyperparamter space and model performances within the context of a broader pipline. \n",
    "\n",
    "**Stage 4: Get Final Results**\n",
    "\n",
    "- Assess differences and drivers of performance by model. This includes an analysis of hyperparameters via model metrics such as F1 beta, precision, and recall as well as feature significance (i.e coefficents, log probabilities, and feature importance).\n",
    "\n",
    "**Stage 5: Present**\n",
    "\n",
    "- Uplevel insights and action items for sharing with non-techinal stakeholders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1b6b850-fa04-438d-84f5-8bb5e6a276c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ea70427-294e-440e-8901-ee04d4c51b9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Besides helping set a baseline, we also experimented with logistic regression to demonstrate the value of our feature engineering efforts and to refine the model, making it competitive in terms of performance, interpretability, and resource usage. \n",
    "We grouped our newly created features into 3 groups and ran logistic regression with forward sequential feature selection to track the effect of each group on the performance of the model. The first group included the historic flight delay (lag) features and the airport traffic volume features, as all of these were numerical and required scaling. The second group included the PageRank feature in addition to those in group 1. The final group included the holiday indicator and the special weather indicator in addition to the features in group 2. Below are the results of these trials, showing an improvement in the performance with each added group of features, and thus value in the features we created, particularly the historic (lag) and airport volume features.\n",
    "\n",
    "| Fbeta  | Baseling | + Historic flights and airport traffic volume | + PageRank | + Holidays and special weather |\n",
    "|------|-------|-----------|--------|--------|\n",
    "| Training | 0.63  | 0.67    | 0.70  | 0.70 |\n",
    "| Validation   | 0.57  | 0.66    | 0.69   | 0.70 |\n",
    "| Change in validation Fbeta   | -  | +0.09 | +0.03  | +0.01 |\n",
    "\n",
    "\n",
    "As some seasonality was already captured in the holiday indicator, we ran another trial without “quarter,” “month,” and “day of month,” showing no difference in performance. We opted to remove these features to further reduce cardinality.\n",
    "\n",
    "We then tuned the hyperparameters of logistic regression with the final set of features through Optuna. This helped determine that an elasticNet mixing parameter of 0.2 (from testing values between 0 to 1) and a regularization parameter of 0 (from testing values of 0.001, 0.01, 0.1, 0, and 1) was best for the performance of our model. \n",
    "\n",
    "Finally, we experimented with adjusting the classification threshold. Knowing that the data was imbalanced against our label of interest and since we weighed precison heavier than recall, we increased the threshold to favor precision and therefore correctly predict more delays. With 10 Optuna trials, we found that a threshold of 0.6 increased precision without significantly hurting recall or affecting fbeta. We thus opted to use this increased classification threshold in our final model.\n",
    "\n",
    "| Training/Validation  | Fbeta | Precision | Recall | \n",
    "|------|-------|-----------|--------|\n",
    "| Threshold = 0.5 | 0.70/0.70  | 0.71/0.71     | 0.63/0.63   |\n",
    "| **Threshold = 0.6**   | 0.70/0.70  | 0.80/0.80      | 0.47/0.47   |\n",
    "| Threshold = 0.7  | 0.63/0.63  | 0.87/0.86      | 0.30/0.30   |\n",
    "\n",
    "Below are the results of the blind testing performed with tuned logistic regression:\n",
    "\n",
    "| Split | fbeta | precision | recall |\n",
    "|-------|-------|-----------|--------|\n",
    "| train | 0.69  | 0.80      | 0.45   |\n",
    "| test  | **0.48**  | 0.50      | 0.44   |\n",
    "\n",
    "This model performed surprisingly well as compared to the baseline when considering its simplicity and quick runtime. With a cluster size of up to 10 workers, 10 Optuna trials took up to 17 minutes to run, with a final inference time of 2 minutes on the blind test set. The option to adjust the classification threshold made it easy to customize the model performance based on our business needs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd3a58e7-dc0b-4b4b-81d2-6ada6c7285e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d0a5e36-bde5-4835-b3b2-606fc092c364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Background**\n",
    "Naive Bayes, like Logistic Regression, provides a fairly robust and explainable algorithm for binary (and multi-class) classifying problems.\n",
    "\n",
    "**Advantages**\n",
    "- Explainable. Conditional probability is much easier to explain to stakeholders than many alternatives (e.g., Neural Nets, tree-based models, etc.)\n",
    "- Established. Bayes has a 300 year history and carries with it a rich history of modern application (e.g., spam detection)\n",
    "\n",
    "**Disadvantages**\n",
    "- Calculating probabilities for every feature and class  means multiple passes\n",
    "- Converting features into probabilities is computationally expensive\n",
    "- Takes a very long time to re-train\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eff8e789-5893-4db6-a23e-b18b8b5d283e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Experimentation\n",
    "\n",
    "**Experimentation limited to hyperparameter optimization.** Given the computational cost/time associated with running Naive Bayes, experimentation was limited to exploring a range of smoothing and imputation options. Optuna was used for the purpose of experimentation as a means to explore and optimize the hyperparameter space, which was limited to testing (1) smoothing and (2) imputation.\n",
    "\n",
    "#### Smoothing\n",
    "\n",
    "**Values between 1.7 and 2.0 were found to be optimal.** Outcomes were relatively consistent across the range.A log range was used to select values for the Optuna study with a range of 0.01 to 10.0. \n",
    "\n",
    "#### Imputation\n",
    "\n",
    "**Mean and Median performance was equal.** Initially, random sampling from distributions representative of the numerical features was considered. However, after deliberation, it was decided that the experiment would use either mean or median. The mean and median were considered to be relatively neutral, as opposited to zero, and used to avoid the scenario in which imputed values contribute a substantial influence to the prediction. The imputation strategy did not show large influence over the performance outcome.\n",
    "\n",
    "#### Scaling\n",
    "\n",
    "**Scaling is not an issue with Bayes.** Given that Bayes works requires the features be represented in terms of conditional probability, scaling was not considered to be necessary. Also, scaling poses a risk of producing negative numbers due to floating error issues.\n",
    "\n",
    "#### Shifting\n",
    "\n",
    "**Negative feature values were an issue.**  In effort to address the incompatibility of negative numbers, the columns were examined for a global minimum value. The difference between this global minimum value and zero was used to then shift the numbers to the right (into the positive space), which included a buffer number. While a practical solution, it presents logistical hurdles. The shift value would have to be updated based on what might be considered reasonable in the test data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8c3c03d-82e9-49c2-a60a-0724b3d5ce55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Discussion of Results\n",
    "\n",
    "Performance was suboptimal, but precision and recall achieved similar scores across each of the datasets (train, val and test) on the 1 year and 5 year datasets. Below is a summary of the model performance for each dataset.\n",
    "\n",
    "**5 year dataset**\n",
    "Similar to the the train and val splits on the 1 year dataset, the test dataset showed higher performance generally on recall. In this case, 4 yrs (2014-2018) was used for training and 2019 was used for the test set.\n",
    "\n",
    "| split  | fbeta | precision | recall |\n",
    "|--------|-------|-----------|--------|\n",
    "| train  | 0.52  | 0.51      | 0.52   |\n",
    "| test   | 0.52  | 0.51      | 0.52   |\n",
    "\n",
    "With a cluster size of up to 10 workers, 10 Optuna trials took up to 14 minutes to run, with a final inference time of 6 minutes on the blind test set.\n",
    "\n",
    "#### Stability\n",
    "\n",
    "Performance across folds was fairly consistent, indicating the performance of the model was relatively stable.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/michaelkalish2008/261_ml-scale/refs/heads/main/new_perf.png\" alt=\"Feature Importance\" width=\"1200\" height=\"1000\">\n",
    "\n",
    "#### Feature importance for 1 year training data\n",
    "\n",
    "Given Naive Bayes does not require the probabilities to be normalized and substitutes logarithmic operations for multiplication, the feature importances can be depicted in terms of logarthimic probabilities (e.g., log P(feature | class). The visualization illustrates the differences between the log probabilities, where positive forms of the differences are expressed by class.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/michaelkalish2008/261_ml-scale/refs/heads/main/importance.png\" alt=\"Feature Importance\" width=\"1000\" height=\"800\">\n",
    "\n",
    "#### Insights\n",
    "\n",
    "Below are a few observations given the performance and influential features:\n",
    "\n",
    "- Arrival/departure block times and historical data, such as HIST_ARR_DELAY_imputed, was (as expected) the most influential for delayed flights\n",
    "- isHoliday and visibility were the leading features for not-delayed flights\n",
    "- Weather data was split across both classes as leading, influential variables\n",
    "- The lack of overfitting suggests that if NB is to be considered a competitative algorithm, despite slightly lower scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d64bf233-e086-4341-879b-a15c1e38853a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9447cdf-b2f8-49e6-94fb-7850daa8959d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Background**\n",
    "\n",
    "Multilayer Perceptrons provide a more complex way to classify flight delays. Each feature is passed in as a node in the input layer and hidden layers can be configured with nodes to perform computations on the weighted inputs through activation functions, allowing the model to learn complex patterns in the data. Finally, the output layer returns the probability of belonging to one of the classes (delayed or not delayed). \n",
    "\n",
    "**Advantages**\n",
    "- Hidden layers can help the model capture nonlinear patterns in the data, something that simpler models like Logistic Regression will be unable to do. This allows the model to better learn interactions between features. \n",
    "- The model is more versatile with being able to experiment with different numbers of layers and neurons, and it scales well with large datasets.\n",
    "\n",
    "**Disadvantages**\n",
    "- It is more computationally expensive than simpler models like Naive Bayes and Logistic Regression and therefore will have a longer runtime to train.\n",
    "- More hyperparamter tuning is required compared to simpler models such as learning rate and hidden layer size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0736bdf7-c25d-4073-a164-566fe8420c31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Experimentation\n",
    "We experimented with two configurations of the Multilayer Perceptron. One configuration had only one hidden layer while the second configuration had two hidden layers. The input layer for both configurations was set to 85 neurons as that was was how many features were returned from the preprocessing pipeline. Other parameters that were held constant in all experiments were the imputation strategy and the scaling strategy. Since the experiments with the baseline model showed that imputing null values with the mean or median did not make a significant difference, we decided to stick to only imputing with the mean to reduce the hyperparamter space for all experiments. Additionally, StandardScaler was applied to the wind direction, temperature, and sea level pressure features, as those already had a fairly normal distribution. MinMaxScaler was applied to the remaining numerical features, as their distributions were heavily skewed. For both configurations, the following hyperparameters were experimented with:\n",
    "- Maximum iterations: 10 to 100\n",
    "- Learning rate: 0.01 to 0.10\n",
    "- Hidden Layer Size for first configuration: 10 to 170 (twice the size of the input layer)\n",
    "- First Hidden Layer Size for second configuration: 10 to 170\n",
    "- Second Hidden Layer Size for second configuration: 10 to 85\n",
    "\n",
    "Ten Optuna trials were ran on each configuration to determine the best hyperparameters for each configuration and below are the results. \n",
    "|  | 1 Hidden Layer  | 2 Hidden Layers | \n",
    "|--------|-------|-----------\n",
    "| Layers | [85, 21, 2]  | [85,14,15,2]      | \n",
    "| Maximum Iterations  | 88 | 82      | \n",
    "| Learning Rate  | 0.015  | 0.012      | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c870eca1-1944-4401-8f32-570a2fe3922f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Discussion of Results\n",
    "Below are the results for the one hidden layer configuration.\n",
    "| split  | fbeta | precision | recall |\n",
    "|--------|-------|-----------|--------|\n",
    "| train  | 0.68  | 0.69      | 0.63   |\n",
    "| test   | 0.38  | 0.35      | 0.61   |\n",
    "\n",
    "Below are the results for the two hidden layers configuration.\n",
    "| split  | fbeta | precision | recall |\n",
    "|--------|-------|-----------|--------|\n",
    "| train  | 0.67 | 0.69      | 0.61   |\n",
    "| test   | 0.38  | 0.35      | 0.59   |\n",
    "\n",
    "Results for both models was fairly similar although the configuration with one hidden layer performed slightly better across all metrics. This indicates that a shallower model with more neurons in its hidden layer was more effective for training and capturing feature interactions without needing additional layers. That being said, performance for both models was still suboptimal compared to the fine-tuned Logistic Regression model. There was a major drop in test performance metrics for fbeta and precision, indicating the model may have overfit on the training data and failed to capture the imbalanced nature of the test set. However, recall was still fairly high for both configurations and closer to the training metric. This was also a more computationally expensive model with a larger hyperparamter search space compared to Naive Bayes and Logistic Regrssion models. Training time for both configurations took close to an hour with 10 workers running, and inference time was about 3 minutes. \n",
    "\n",
    "####Stability\n",
    "Performance across folds for the one hidden layer configuration was fairly consistent, indicating the performance of the model was relatively stable. The biggest differences between the final training metrics and the fold metrics occured at fold 2.\n",
    "<img src=\"https://raw.githubusercontent.com/Eshan-Bhatnagar/261_project/refs/heads/main/mlpStability.png\">\n",
    "\n",
    "Performance across folds for the two hidden layers configuration was a little less consistent compared to the one hidden layer configuration, but differences between the final training metrics and fold metrics were not significant enough to indicate the model was unstable. Major differences occured at folds 2 and 5, especially for the recall metrics.\n",
    "<img src=\"https://raw.githubusercontent.com/Eshan-Bhatnagar/261_project/refs/heads/main/mlp2Stability.png\">\n",
    "\n",
    "####Feature Importance\n",
    "Looking at the feature importances for the one hidden layer configuration, we can see that gust speed was the most influential feature for predicting delays with other weather based features such as wind speed, dew point, and ceiling height also having influence, as well as the historical departure delay. On the other hand, features such as unique carriers, historical arrival delays, and the month were more influential on non delayed flights, which went against some of our expectations from the EDA of these features.\n",
    "<img src=\"https://raw.githubusercontent.com/Eshan-Bhatnagar/261_project/refs/heads/main/mlpFeatures.png\">\n",
    "\n",
    "The feature importances of the two hidden layer configuration yielded much different results. Time based features such as day of week and departure time block were more influential on predicting delays whereas weather based features such as dew point and visibility were more important for the non delayed class. \n",
    "<img src=\"https://raw.githubusercontent.com/Eshan-Bhatnagar/261_project/refs/heads/main/mlp2Features.png\">\n",
    "\n",
    "####Insights\n",
    "Below are a few take aways given the performance and influential features:\n",
    "- Unlike the Naive Bayes and Logistic Regression models which were more reliant on time based and lagged features, weather based features (which made up the majority of our numerical data) had a stronger influence on both classes for this model. \n",
    "- The differences in feature importance between the one hidden layer and two hidden layer configurations shows how fickle the model is to hyperparameter tuning and the configuration of its hidden layers. Further experimentation with these parameters could help improve model performance and define a solid set of influential features. \n",
    "- Despite being a more complex and computationally expensive model, the multilayer perceptron did not perform as well as Logistic Regression although it performed fairly well for recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c2c07b1-03cc-4994-a7f6-2ca8263d4721",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "867ac5bf-b407-4269-9c8b-eb7d9d8f1d9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Background**\n",
    "Random Forest is a very flexible and robust model for both regression and classification problems.\n",
    "\n",
    "**Advantages**\n",
    "- Transparent. Decision pathways can be extracted and examined\n",
    "- Widely Used. Amongst the most frequently used and robust models currently in use\n",
    "- Flexible. Capable of modeling more complex sets of data with success\n",
    "- Minimal Preprocessing. Don't require features to be scaled or adjusted relative to one another, speeding up modeling pipelines\n",
    "- Quick. Fast to train\n",
    "\n",
    "**Disadvantages**\n",
    "- Prone to overfitting, must make sure to leverage pruning, bagging, cross-validation, etc to ensure reasonable abstraction of the model \n",
    "- Extensive hyperparameter space is computationally expensive to traverse\n",
    "- Larger forests can be trickier to understand\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1187bd2b-f289-4dca-9c5b-aefe9549a57a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Experimentation\n",
    "\n",
    "**Experimentation limited to hyperparameter optimization.** As mentioned earlier Random Forest had a large hyperparameter search space. To avoid running an intensive gridsearch we utilized Optuna to more effectively traverse the hyperparameter space which included the following parameters: num_trees, max_depth, feature_subset_strategy, min_instances_per_node, max_bins, impurity, gini, and min_info_gain. Furthermore, for the 5 year dataset we implemented cross validation to get a weighted F1 score.\n",
    "\n",
    "As for the data being used in the trial, we held constant in all experiments were the imputation strategy and the scaling strategy with imputing null values with the mean and applying StandardScaler wind direction, temperature, and sea level pressure features, as those already had a fairly normal distribution and MinMaxScaler to the remaining numerical features, as their distributions were heavily skewed.\n",
    "\n",
    "#### Number of Trees\n",
    "\n",
    "**Optimal number of trees found to be 23.** Testing in the range of 10 to 30 trees, we found that the optimal number of trees near the middle at 23. We consistently observed that forests with more estimators generally performed better on our dataset. This was a key hyperparameter!\n",
    "\n",
    "#### Max Depth\n",
    "\n",
    "**Optimal max depth found to be 19.** Testing in the range of 3 to 20, we found that the optimal max depth to range from 14 to 19. Given the complexity of the interations needing to be modeled, it makes sense that a higher max depth would be benifitial. This was a key hyperparameter!\n",
    "\n",
    "#### Feature Subset Strategy\n",
    "\n",
    "**Optimal feature subset strategy found to be log2.** Testing included \"auto\", \"sqrt\", and \"log2\" but we generally saw best results with \"log2\" and some comparable results with \"sqrt\". This parameter determines the strategy for choosing the number of randomly selected variables for modeling (i.e given 100 features log(100) ~ 6 vs sqrt(100) = 10). \n",
    "\n",
    "##### Min Instaces Per Node\n",
    "\n",
    "**Optimal number of min instances needed per node was 14.** So the min instanes determine the number of observations necessary to create a leaf node or a point at which the model decides to post a prediction. We tested values in the range of 1 to 20 and found that values leaning towards the upper limit performed better, likely because they were less prone to overfitting by creating a node for every sample. This was a key hyperparameter!\n",
    "\n",
    "#### Max Bins\n",
    "\n",
    "**Optimal number of max bins was 96.** Max bins will group values for continious features into bins to make them easier to manage while preserving trends. There is a trade off between sparseness and granularity here, as having too many bins will lead to many bins having no values but aving too few bins will oversimplify the data and lose trends. From the range we tested, 16 to 128 in steps of 8, we found 96 to be the best balance for our features.\n",
    "\n",
    "#### Impurity\n",
    "\n",
    "**Optimal impurity strategy was gini.** Impurity refers to how the decision of what feature to split upon are made. The choice of impurity score strategy was between 'gini' and 'entropy' and we found that 'gini' typically worked best.\n",
    "\n",
    "#### Min Info Gain\n",
    "\n",
    "**Optimal min info gain was 0.** In our trials we tested values from 0 to 1 and found 0 to be the ideal value, likely beacuse the Min Instances per node value being higher eliminated the need to test for meaningful information gain at very low levels since we stopped before we got to overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7441dc7f-e7a6-43f5-9ec0-2b497be52b9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Discussion of Results\n",
    "Below are the results for the optimal random forest configuration.\n",
    "| split  | fbeta | precision | recall |\n",
    "|--------|-------|-----------|--------|\n",
    "| train  | 0.74  | 0.78      | 0.61   |\n",
    "| test   | 0.48  | 0.46      | 0.59   |\n",
    "\n",
    "Performance for the fine-tuned Random Forest model was similar to the fine-tuned Logistic Regression model. There was a major drop in test performance metrics for fbeta and precision, indicating the model may have overfit on the training data and failed to capture the imbalanced nature of the test set. However, recall was still fairly high and closer to the training metric. Random forests were also a more computationally expensive model with a larger hyperparamter search space compared to Naive Bayes and Logistic Regrssion models. Training time  took close to 8 minutes with 10 workers running, and inference time was about 4 minutes.\n",
    "\n",
    "####Stability\n",
    "Performance across folds was fairly consistent, indicating the performance of the model was relatively stable. All the folds being similar, yet the testing metrics being different signals that there was definitely some over fitting occuring. This is something that we could address using additional measures such as pruning and bagging.\n",
    "<img src=\"https://raw.githubusercontent.com/ANRC2020/w261-Final-Project/refs/heads/main/w261%20Random%20Forest%20Cross-Fold%20Performance%20-%20Copy.jpg\">\n",
    "\n",
    "####Feature Importance\n",
    "Looking at the feature importances for the the fine-tuned random forest, we can see that historal features lead as the most important. Additionally, we can see that time blocks, distance traveled and time elasped were also quite important. We see that attributes about the course of a flight and historical attributes of that flight are the most important for differentiating between non-delayed and delayed flights moreso than weather features.\n",
    "<img src=\"https://raw.githubusercontent.com/ANRC2020/w261-Final-Project/refs/heads/main/w261%20Random%20Forest%20Feature%20Importance.jpg\">\n",
    "\n",
    "\n",
    "####Insights\n",
    "Some take aways given the performance and influential features:\n",
    "- Like the Naive Bayes and Logistic Regression models, Random forest was more reliant on time based and lagged features related to the flight over weather based features.\n",
    "- Random Forest did have one of the higher F1 beta testing scores but significant drop off from its validation metrics. Given more time it would be worth exploring additional strategies for preventing overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c5c986a-6bf4-440f-b78d-7f5f7569cf95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb192ed7-9f68-40be-b138-e17c812caeb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Background**\n",
    "While gradient boosting trees are ensemble learning methods that contain a similar tree structure algorithm as random forest, it is different by the fact that each tree is built sequentially in attempt to correct the errors of the previous tree. This provides a more focused approach to iteratively improving a model's weaknesses. The effect is reduced bias and improved accuracy. \n",
    "\n",
    "**Advantages**\n",
    "- Flexible: capable of Handling imbalanced datasets and missing data.\n",
    "- Capable of handling non-linear complex relationships.\n",
    "- High Accuracy: by sequentially adding trees to correct errors.\n",
    "\n",
    "**Disadvantages**\n",
    "- Computationally expensive and slow for large datasets.\n",
    "- More prone to overfitting (even compared to random forests)\n",
    "- Not as interpretable as random forests or decision trees.\n",
    "- Sensitive to outliers\n",
    "- Extensive hyperparameter space is computationally expensive to traverse and optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acbabb09-f1a4-4790-baf7-321e6f480415",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Experimentation\n",
    "\n",
    "**Experimentation limited to hyperparameter optimization.** As mentioned earlier Boosted Trees have a large hyperparameter search space, even larger than random forests. To avoid running an intensive gridsearch we utilized Optuna to more effectively traverse the hyperparameter space which included the following parameters: n_estimators, max_depth, subsample, learning rate, booster, gamma, and min_child_weight. Furthermore, for the 5 year dataset we implemented cross validation to get a weighted F1 score.\n",
    "\n",
    "**As for the data being used in the trial, we held constant in all experiments were the imputation strategy and the scaling strategy with imputing null values with the mean and applying StandardScaler wind direction, temperature, and sea level pressure features, as those already had a fairly normal distribution and MinMaxScaler to the remaining numerical features, as their distributions were heavily skewed.**\n",
    "\n",
    "#### n_estimators (number of trees)\n",
    "\n",
    "**Optimal number of trees found to be 29.** Testing in the range of 10 to 30 trees, we found that the optimal number of trees on the lower end of the tested feature space at 29. We consistently observed that boosted trees with more estimators generally performed better on our dataset. This was a key hyperparameter!\n",
    "\n",
    "#### Max Depth\n",
    "\n",
    "**Optimal max depth found to be 19.** Testing in the range of 3 to 20, we found that the optimal max depth to be 14. Given the complexity of the interations needing to be modeled, it makes sense that a higher max depth would be benifitial. This was a key hyperparameter!\n",
    "\n",
    "#### Subsample\n",
    "\n",
    "**Optimal subsample ratio found to be 0.48.** Testing in the range of 0.1 to 1.0, we found that the optimal subsample to be 0.48. This means that about half of our training data was subsampled each time the algorithm trained a new tree. Furthermore, by subsampling for training we are introducing randomness and hopefully preventing overfitting. \n",
    "\n",
    "#### Booster\n",
    "\n",
    "**Optimal booster method found to be dart.** Tested algorithms `gbtree` and `dart`. In general, these methods identify how the progression of boosted trees can be improved. `gbtree` provides gradients in between boosting to improve subsequent trees, while `dart` provides gradients but also removes ineffective trees to help manage overfitting. Our validation, identified `dart` as being most effective.\n",
    "\n",
    "#### Gamma\n",
    "\n",
    "**Optimal gamma value found to be 6.95.** Testing in the range of 0.0 to 10.0, we found that the optimal subsample to be 6.95. The optimal value was on the higher side of the provided range, indicating the complexity of the underlying data. In order to prevent over fitting, its important to have a higher threshold for creating a new leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f709a84-0679-4296-991c-9e9a39e0db04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Discussion of Results\n",
    "Below are the results for the optimal random forest configuration.\n",
    "| split  | fbeta | precision | recall |\n",
    "|--------|-------|-----------|--------|\n",
    "| train  | 0.76  | 0.78      | 0.67   |\n",
    "| test   | 0.23  | 0.20      | 0.98   |\n",
    "\n",
    "Performance for the XGBoost model was the poorest of all the models when considering fbeta, precision, and overfitting. While the training evaluation metrics showed promise, the testing metrics were significantly below our OKRs. Boosted trees are prone to overfitting and can require significant tuning to achieve optimal results. From a stability perspective, XGBoost was one of the most stable models during cross-validation as shown in the table below. However, it had a somewhat different selection of the most important features, further highlighting concerns of using this model as has been implemented.\n",
    "#### Stability\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/rmower90/w261_project_images/refs/heads/main/xgboost_folds.jpg\">\n",
    "\n",
    "\n",
    "#### Feature Importance\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ANRC2020/w261-Final-Project/refs/heads/main/w261%20XGBoost%20Feature%20Importance.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1facd9a-92fe-4da9-b0a9-b5180bee144a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Discussion\n",
    "Based on the performance of models tested in this project (Logistic Regression, Naive Bayes, MLP, Random Forest, XGBoost), there are four factors that influenced our decision to prioritize Naive Bayes.\n",
    " \n",
    "**1) Evidence of Stability:**\n",
    "Naive Bayes showed small variance across performance metrics for each individual fold. This suggests a high degree of stability (standard deviation of 1.1% across folds), which will allow it to generalize more reliably for our customers. None of the other models achieved this level of stability (e.g. XGBoost with a standard deviation of 1.3%). Given more time we would like to further tune and improve our models with poor stability.\n",
    "\n",
    "**2) Lack of overfitting:**\n",
    "While we saw some reasonable levels of stability across the Optuna studies, the performance of Naive Bayes on an imbalanced test set was the only model to perform with some degree of consistency. The other models showed substantial overfitting, despite having slightly higher performance metrics. It is our preference to select a model that can generalize well, even if it is at the expense of a slightly improved set of performance metrics.\n",
    "\n",
    "**3) Established history of application:**\n",
    "Naive Bayes has a three hundred year history. It has been used by well-established technology businesses making it a reliable, well-supported and documented algorithm for large-scale production-level inference.  \n",
    "\n",
    "**4) Highly Explainable:**\n",
    "The flight delay inference problem can be expressed as a problem of conditional probability, which suites the character of Naive Bayes. By explaining Naive Bayes in terms of the selected features, non-technical stakeholders can be provided a general accounting of the algorithm. For eample, explaining Bayes can be as follows: What is the probability of a delayed flight, given feature1, feature2, feature3...featuren. These features can additional be explained in terms of their log probabilities, showing non-technical stakeholders the significance of relatable features for the purpose of inferring delays.\n",
    "\n",
    "**5) Additional Labeling:**\n",
    "While our problem was binary in nature, there may be opportunities in the future for a multiclass problem, in which case a set of more nuanced labeling scheme may be applicable. Multinomial Naive Bayes would provide a seamless transition.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Eshan-Bhatnagar/261_project/refs/heads/main/final_metrics.jpg\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "180a0fc1-3d3e-40d1-85d3-ccb5f21c0f0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b124aa8-6956-420c-9a56-0cf3f2f35273",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Phase 3 Credit Assignment Plan\n",
    "## Phase Leader Plan\n",
    "| Phase | Week | Leader |\n",
    "|----------|----------|----------|\n",
    "| 1    | 1     | Michael |\n",
    "| 2    | 2     | Abbas |\n",
    "| 2    | 3     | Ross |\n",
    "| 3    | 4     | Dina |\n",
    "| 3    | 5     | Eshan |\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ANRC2020/w261-Final-Project/refs/heads/main/w261%20ML%20Workload.jpg\" alt=\"Project Workload\" width=\"1200\" height=\"1000\">\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ANRC2020/w261-Final-Project/refs/heads/main/w261%20ML%20Workload%202.jpg\" alt=\"Project Workload\" width=\"1200\" height=\"1000\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a92f79b-17aa-44df-94c2-c05b21c203fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "- **Lagged variables were very important!** Historical data, engineered via lagging, were consistently found to add substantial value in terms of feature importance. This suggests that engineering more features from historical trips may contribute additional value and are worth further exploring. \n",
    "- **We are not in the excellent range.** With this in mind, a pragmatic approach is still possible. To minimize the impact of false positives, the business is advised to qualify expected delays with a disclaimer that this expectation may be subject to change so passengers should not be too far from the gate.\n",
    "- **Overfitting is a problem.** The majority of modeling pipelines indicated substantial overfitting, which suggests there may be difficulty in genralizing their performance. \n",
    "- **Select a model that minimizes overfitting.** We can select a model that is slightly lower performing with less overfitting and this will help ensure generalizability at the expense of potential performance gains. \n",
    "- **An ensemble approach may reduce overfitting and improve results.** Different voting schemes that optimize for a collective performance of models could leverage minority, majority, or weighted voting. This will take additional time and resources that were not avalible within the scope of this project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "431eae0e-da1f-469c-b8fc-e05242f7bbc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Reference Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d34a69c-e563-4cc3-88cb-afb15f811c32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "EDA: https://adb-4248444930383559.19.azuredatabricks.net/editor/notebooks/46527914086190?o=4248444930383559%3Fo%3D4248444930383559#command/2276882173339206\n",
    "\n",
    "Pre-Processing Pipeline: https://adb-4248444930383559.19.azuredatabricks.net/editor/notebooks/57921511667556?o=4248444930383559#command/57921511670848\n",
    "\n",
    "New Feature Creation Pipeline: https://adb-4248444930383559.19.azuredatabricks.net/editor/notebooks/59813923268184?o=4248444930383559#command/59813923268187 \n",
    "\n",
    "Cross-Validation Fold Creation: https://adb-4248444930383559.19.azuredatabricks.net/editor/notebooks/3948917746255337?o=4248444930383559 \n",
    "\n",
    "Baseline/Logisitic Regression: https://adb-4248444930383559.19.azuredatabricks.net/editor/notebooks/59813923271356?o=4248444930383559#command/57921511675462 \n",
    "\n",
    "Naive Bayes: https://adb-4248444930383559.19.azuredatabricks.net/editor/notebooks/59813923271505?o=4248444930383559#command/59813923271554\n",
    "\n",
    "Multilayer Perceptron (1 Hidden Layer Configuration): https://adb-4248444930383559.19.azuredatabricks.net/editor/notebooks/59813923261447?o=4248444930383559\n",
    "\n",
    "Multilayer Perceptron (2 Hidden Layers Configuration): https://adb-4248444930383559.19.azuredatabricks.net/editor/notebooks/57921511668455?o=4248444930383559\n",
    "\n",
    "Random Forest: https://adb-4248444930383559.19.azuredatabricks.net/editor/notebooks/57921511666354?o=4248444930383559\n",
    "\n",
    "XGBoost: https://adb-4248444930383559.19.azuredatabricks.net/editor/notebooks/3061047499229842?o=4248444930383559"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Team_3_1_Phase_3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}